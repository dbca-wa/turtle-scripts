---
title: "Ningaloo Turtle Program Data Extraction, Transformation and Loading"
author: "Florian Mayer"
date: "14 July 2016"
output: html_document
---
This workbook executes and documents the extraction, transformation and loading
(ETL) of Ningaloo Turtle Program data from the original Access database into
accessible formats. Data are read from a snapshot of the original database on
the internal data catalogue, processed locally, and uploaded as various outputs
back to the data catalogue.

**Note** The data shown are not yet fully quality controlled, and therefore
could contain duplication or gaps. This notebook currently is a work in progress.

# Install
Install required packages by running `install.R` once in your Ubuntu environment.
Note: root access is required to install the Ubuntu system package "mdbtools".
```{r, eval=FALSE}
source("install.R")
```

# Setup
Configure `ckanr` for use with our data catalogue.
The file `setup.R` contains the confidential CKAN API key, which gives
the owner's write permissions: `ckanr::ckanr_setup(url=CKAN, key=APIKEY)`.

Create your own `setup.R` from the template `setup_template.R`.
Then, load required libraries and source `setup.R`.

```{r r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/projects/turtle-scripts/ningaloo")
if (file.exists("../config/setup.R")) source("../config/setup.R")
if (file.exists("../config/load.R")) source("../config/load.R")
library(RODBC)
refresh_data <- TRUE # download fresh database snapshot or use local copy
```


# Extract data
Download, unzip and open the Access mdb file from the data catalogue.
```{r, echo=T, message=FALSE}
if (refresh_data) {
  tmp <- tempfile()
  download.file(resource_show(MDB_RID)$url, tmp)
  dbfile <- unzip(tmp, "ningaloov6.mdb", exdir="data")
  unlink(tmp)
} else {
  dbfile <- "data/ningaloov6.mdb"
}
con <- mdb.get(dbfile, dateformat='%Y-%m-%d', as.is=T)
contents(con)
```

# Transform

## Sites
In `tblSections`, sites are called "subsections", representing walkable parts
of a beach.

We'll preserve the centroid coordinate pair as "lat" and "lon", and
the bounding box extent as max and min x and y coordinates.
```{r}
sites <- con$tblSections %>%
  transmute(
    id=as.numeric(SubSect.Id),
    division=as.character(division.name),
    section=as.character(txtSections),
    subsection=as.character(txtSubSection),
    lat=-as.numeric(SubSect.center.lat),
    lon=as.numeric(SubSect.center.long),
    y_max=-as.numeric(SubSect.NE.lat),
    y_min=-as.numeric(SubSect.SW.lat),
    x_max=as.numeric(SubSect.NE.long),
    x_min=as.numeric(SubSect.SW.long))
row.names(sites) <- sites$id

# Example: fix missing NE corner of Red Bluff
# Note: this is a temporary measure until the Access db is updated
# sites[which(sites$subsection=="Red Bluff"),]$x_max <- 113.458
# sites[which(sites$subsection=="Red Bluff"),]$y_max <- -24.016

write.csv(sites, file = "data/sites.csv", row.names=F)
```

### Shapefiles - R style
Following Stackoverflow user [jbaum](http://stackoverflow.com/users/489704/jbaums)'s
[example](http://stackoverflow.com/a/26620550/2813717), we create the R equivalent
of a polygon shapefile.

`coords` is a matrix containing the five coordinate pairs (columns) for all sites (rows).
```{r}
coords <- sites %>%
  transmute(
    sw.lon=x_min, sw.lat=y_min,
    se.lon=x_max, se.lat=y_min,
    ne.lon=x_max, ne.lat=y_max,
    nw.lon=x_min, nw.lat=y_max,
    end.lon=x_min, end.lat=y_min) %>%
  as.matrix()
```

`ids` is a vector containing the site IDs.
```{r}
ids <- sites %>% select(id) %>% as.matrix()
```

`make_polygons` creates one `Polygon` object from a matrix of coordinates and a
vector of IDs like the above.

The matrix of coordinates needs to contain a list of lon/lat pairs with the last one
being a copy of the first one. E.g., a rectangle needs to be represented as
five points: SW, SE, NE, SW, SW to close the ring.
```{r}
#' Create SpatialPolygons from a vector of lon/lat coordinates (poly) and IDs
make_polygons <- function(poly, id) {
  Polygons(list(Polygon(matrix(poly, ncol=2, byrow=TRUE))), ID=id)}
```

Create SpatialPolygons (SpatialPolygons `polys`) from the coordinates
(matrix `coords`) and IDs (matrix `ids`) by vectorising the function
`make_polygons` over all sites (one site is one row in `coords`) using the
correct projection (WGS84).
```{r}
wgs84 <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
polys <- SpatialPolygons(mapply(make_polygons, split(coords, row(coords)), ids), proj4string=wgs84)
```

Create a SpatialPolygonsDataFrame `polys.df` from SpatialPolygons `polys` and a
data.frame `sites`. This is the R version of a shapefile, containing spatial
features (`polys`) as well as text attributes (`sites`).
```{r}
polys.df <- SpatialPolygonsDataFrame(polys, as.data.frame(sites))
```

Write the SPDF to GeoJSON and preview them on an interactive map.
```{r}
writeOGR(polys.df, "data/sites.geojson", layer="geojson", driver="GeoJSON")
mapview(polys.df)
```

## Surveys
Surveys are joined to environmental condition, column names are sanitised.
Read `tblDBAreaSurveyed`, parse date formats, infer timezone GMT+08, resolve
lookup for Yes/No, and clean up column names.

**Note** The table `tblDBAreaSurveyed` lacks a foreign key `site_id` to
`tblSections`, instead it repeats division, section and subsection names.

Since there are two subsections with different divisions (site_id 64 and 68),
joining by subsection alone will result in duplicate surveys.

We will break good database practice and, for lack of a proper foreign key,
join surveys to sites by all of division, section and subsection names.

The boolean indicator for disturbance `Ysn.id` is 1 for yes (good practice) and 2
for no (normally 0 means no). We will keep the 1 for yes and change all occurrences
of the number 2 to 0 to align this field with the other boolean fields.

The seasons begin after October and end before April, so we'll assume that we can
infer the season by a watershed month of July - e.g. observations betweeen
Aug and Dec 2012 will be considered season 2013-2014, while observations between
Jan and Jul 2012 will be considered seasin 2011-2012. Therefore, we won't need the
seasons table at all.

```{r}
ord <- c("mdyHMS")
tz <- "UTC"

surveys <- con$tblDBAreaSurveyed %>%
  left_join(con$tblEnvironCond, by="date.id") %>%
  mutate(
    date.id=parse_date_time(date.id, ord, tz),
    time.HT=parse_date_time(time.HT, ord, tz),
    time.HT=date.id + hour(time.HT) + minute(time.HT) + second(time.HT),
    season=ifelse(month(date.id) > 7, year(date.id), year(date.id)-1)
    ) %>%
  rename(
    date=date.id,
    survey_id=area.svyd.id,
    division=Divsion,
    section=Section,
    subsection=SubSection,
    site_disturbed=Ysn.id,
    fox_tracks_present=FoxTracks,
    dog_tracks_present=DogTracks,
    no_false_crawls_fox_tracks=numFalseCrawlsFoxTracks,
    survey_comments=comments,
    wind_speed=wind.speed,
    wind_direction=wind.direction,
    air_temp=air.temp,
    water_temp=water.temp,
    time_of_high_tide=time.HT,
    height_of_high_tide=hght.HT
    ) %>%
  left_join(sites, by=c("division","section", "subsection"))

# Fix non-standard boolean values for yes/no lookups
surveys[which(surveys$site_disturbed==2),]$site_disturbed=0

write.csv(surveys, file = "data/surveys.csv", row.names=F)
# DT::datatable(surveys)
```

## Lookups
Let's clean up the column names of lookups.
```{r}
species <- rename(con$tblTurtleSpecies, species_id=Turtle.Species.ID, species_name=Turtle.Species.Name)
nest_types <- rename(con$tblNestType, nest_type_id=NestTypeID, nest_type=NestType)
confidence <- rename(con$tblPosConf, confidence_id=PosConf.ID, confidence=txtPosConf)
position <-  rename(con$tblProfilePos, position_id=intPosID, position=Position)
track_type <- rename(con$tblTrackType, track_id=track.id, track_origin=txtTrackName)
```

## Observations: False crawls
Munge `tblDBFalseCrawl`:

* Clean up column names,
* drop duplicated "division" and "section",
* resolve species lookups,
* append survey level data,
* replace the missing species names (NA) with "NA",
* write to CSV and preview.

```{r}
crawls <- con$tblDBFalseCrawl %>%
  rename(
    species_id=FalseCrawlSpecies,
    survey_id=area.svyd.id,
    no_false_crawls=NumberFalseCrawls,
    crawl_id=FalseCrawlRecordID) %>%
  left_join(species, by="species_id") %>%
  select(-species_id) %>%
  left_join(surveys, by="survey_id")
crawls[which(is.na(crawls$species_name)),]$species_name = 'NA'
write.csv(crawls, file = "data/crawls.csv", row.names=F)

# DT::datatable(crawls)
```

## Observations: Real nests
Like with false crawls, munge `tblDBNestingSurvey`:

* sanitise column names,
* resolve all lookups and drop their purely internal IDs,
* restore the missing negative latitude sign,
* append survey level data,
* reorder and select columns,
* set row names to nest id,
* restore boolean nest disturbance values,
* save to CSV and preview.

```{r}
nests <- con$tblDBNestingSurvey %>%
  mutate(
    longitude=as.numeric(longitude),
    latitude=-as.numeric(latitude)) %>%
  rename(
    nest_id=NestID,
    survey_id=area.svyd.id,
    nest_type_id=NestType,
    confidence_id=PosConf.ID,
    position_id=intPosID,
    nest_disturbed=ysnNestDist.ID,
    species_id=crawl.id,
    track_id=track.id,
    track_id2=track.id2,
    camera_photo_no=CameraPhotoNo,
    comments=Comments) %>%
  left_join(nest_types, by="nest_type_id") %>%
  left_join(confidence, by="confidence_id") %>%
  left_join(position, by="position_id") %>%
  left_join(track_type, by="track_id") %>%
  left_join(species, by="species_id") %>%
  inner_join(surveys, by="survey_id") %>%
  select(
    survey_id, nest_id, division, section, subsection,
    date, season, longitude, latitude,
    species_name, nest_type, position, confidence,
    track_origin, track_id, track_id2,
    nest_disturbed, fox_tracks_present, dog_tracks_present,
    camera_photo_no, comments, survey_comments,
    wind_speed, wind_direction, air_temp, water_temp,
    time_of_high_tide, height_of_high_tide, lat, lon)
row.names(nests) <- nests$nest_id
nests[which(nests$nest_disturbed==2),]$nest_disturbed=0
write.csv(nests, file = "data/nests.csv", row.names=F)
# DT::datatable(nests)
```

### Summary of new nests by survey
The sum of new nests for every unique combination of survey (subsection, date)
and species.

Steps:

* Filter the nest observations to new nests only
* Group by unique combinations of subsection and date (= surveys) and species name
* Count the observations of each group (= total number of nests of species x
  seen on date y at subsection z)
* Append survey level data
* Save to CSV and show as interactive datatable

```{r}
summary_nests <- tbl_df(nests) %>%
  filter(nest_type=="New") %>%
  group_by(subsection, date, species_name) %>%
  tally(sort=T) %>%
  ungroup() %>%
  spread(species_name, n, fill=0) %>%
  inner_join(surveys)
write.csv(summary_nests, file = "data/summary_nests.csv", row.names=F)
DT::datatable(summary_nests)
```


```{r}
summary_nests_seasons <- tbl_df(nests) %>%
  filter(nest_type=="New") %>%
  group_by(subsection, season, species_name) %>%
  tally(sort=T) %>%
  ungroup() %>%
  spread(species_name, n, fill=0) %>%
  inner_join(sites, by="subsection") %>%
  select(-x_max, -x_min, -y_max, -y_min)
write.csv(summary_nests_seasons, file = "data/summary_nests_seasons.csv", row.names=F)
DT::datatable(summary_nests_seasons)


# Follow http://environmentalinformatics-marburg.github.io/mapview/popups/html/popups.html
# to show subsection summaries over subsection polygons on map
#
# make_popup <- function(subsec){
#   htmlTable(
#     summary_nests_seasons %>%
#     filter(subsection==subsec) %>%
#       select(subsection, season, Green, Loggerhead, Hawksbill, Flatback, Unidentified) %>%
#       arrange(subsection, season)
#     )
#   }
# sites <- mutate(sites, popup=make_popup(subsection))
# row.names(sites) <- sites$id
# polys.df <- SpatialPolygonsDataFrame(polys, as.data.frame(sites))
# writeOGR(polys.df, "data/sites.geojson", layer="geojson", driver="GeoJSON")
# mapview(polys.df)

```
Be aware that the 278 season / subsection summaries are duplicated partially
for "Baiting 1080" site duplicates.

### False crawls by survey
The sum of false crawls for every unique combination of survey (subsection, date)
and species.

The steps are similar to the nest summary table.

```{r}
summary_crawls <- tbl_df(crawls) %>%
  group_by(survey_id, species_name) %>%
  tally(sort=F) %>%
  ungroup() %>%
  spread(species_name, n, fill=0) %>%
  inner_join(surveys) %>%
  select(-x_max, -x_min, -y_max, -y_min)
write.csv(summary_crawls, file = "data/summary_crawls.csv", row.names=F)
DT::datatable(summary_crawls)
```

# QA
The following section will assist in the quality assurance of the data.

### Duplicated Sites
```{r}
duplicated_sites <- sites %>% filter(duplicated(sites$subsection))
DT::datatable(duplicated_sites, caption = "Duplicated sites")
```

### Sites with missing coordinates
Although we can patch the data here, sites with missing coordinates need to be fixed in the original data source!
If there are any sites listed here, please supply the missing data in the original
NTP access database, then zip and upload the original Access database to the data catalogue.
```{r}
badsites <- filter(con$tblSections,
  is.na(SubSect.NE.lat) | is.na(SubSect.NE.long) |
    is.na(SubSect.SW.lat) | is.na(SubSect.SW.long))
DT::datatable(badsites, caption="Sites with missing coordinates")
```

### Observations against missing surveys
There are some observations of crawls (currently 302) and nests (currently 299)
referring to non-existing survey ids.

Above products `nests`, `crawls` and the summaries currently exclude observations
referring to non-existant surveys.

Missing surveys should be entered at source if possible.

```{r}
crawls_missing_survey <- anti_join(crawls, surveys, by="survey_id")
DT::datatable(crawls_missing_survey[,1:5])

nests_missing_survey <- anti_join(nests, surveys, by="survey_id")
DT::datatable(nests_missing_survey[,1:12])
```

### Crawl observations missing species name
22 observations have NA values for their species name lookup.
These have been replaced with species name "NA" for the above products,
but should be fixed at source.

```{r}
cr <- con$tblDBFalseCrawl %>%
  rename(
    species_id=FalseCrawlSpecies,
    survey_id=area.svyd.id,
    no_false_crawls=NumberFalseCrawls,
    crawl_id=FalseCrawlRecordID) %>%
  left_join(species, by="species_id") %>%
  select(-species_id) %>%
  left_join(surveys, by="survey_id")
DT::datatable(filter(cr, is.na(species_name))[,1:8])
```

# Load
Upload outputs to CKAN.
```{r}
# ckanr::resource_update(SITES_RID, path="data/sites.geojson")
# ckanr::resource_update(SITES_CSV_RID, path="data/sites.csv")
# ckanr::resource_update(SURVEYS_RID, path="data/surveys.csv")
# ckanr::resource_update(CRAWL_RID, "data/crawls.csv")
# ckanr::resource_update(NEST_RID, "data/nests.csv")
# ckanr::resource_update(NEW_NEST_RID, "data/summary_nests.csv")
# ckanr::resource_update(NEST_SEASON_RID, "data/summary_nests_seasons.csv")
# ckanr::resource_update(FALSE_CRAWL_RID, "data/summary_crawls.csv")
```

# Selecting data
Data are only of interest for further analysis if the survey effort has been
comprehesive enough.

In particular, the only relevant surveys are those from comprehensively surveyed sections.

For each survey day, we want to know which sections to exclude.
from surveys: date, section, tally of subsections
vs
frim sites: section, tally of subsections in section


```{r}
subsection_count <- sites %>%
  group_by(section) %>%
  tally(sort=F) %>%
  rename(no_subsections_existing = n)
subsection_count

survey_completeness <- surveys %>% group_by(section, date) %>%
  tally(sort=F) %>%
  ungroup() %>%
  filter(!is.na(section) & section!="") %>%
  left_join(subsection_count, by = "section") %>%
  filter(!is.na(no_subsections_existing)) %>%
  rename(no_subsections_surveyed = n) %>%
  mutate(all_surveyed = ifelse(no_subsections_surveyed < no_subsections_existing, F, T))
DT::datatable(survey_completeness)
```

Next:

* join `summary_nests` with `survey_completeness`
* s1 = `all_surveyed`==T, Only surveys of sections which had all of their 
  subsections surveyed on one day
* s2 = `all_surveyed`==F, All but the above

Nest and crawl tallies for s1 per day and division, per section (are they moving?)


```{r}
setwd("~/projects/turtle-scripts")
```
